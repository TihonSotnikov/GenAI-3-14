# GenAI-3-14: Персонализированный чат-бот с памятью

![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Technology](https://img.shields.io/badge/Ollama%20&%20Llama%203-orange.svg)

Консольный чат-бот, демонстрирующий систему генерации персонализированных ответов с использованием локальной языковой модели. Приложение сохраняет профиль пользователя (имя, интересы) и использует его вместе с историей диалога для создания контекстно-зависимых и индивидуализированных реплик.

---

## 📋 Содержание

- [Возможности](#-возможности)
- [Установка](#-установка)
- [Использование](#-использование)
- [Настройка](#-настройка)
- [Структура проекта](#-структура-проекта)
- [Пример сессии](#-пример-сессии)
- [Возможные улучшения](#-возможные-улучшения)
- [Образовательные цели](#-образовательные-цели)
- [Лицензия](#-лицензия)

---

## ✨ Возможности

- **Управление профилем** — создание профиля пользователя с его именем и интересами при первом запуске.
- **Персонализация ответов** — динамическая генерация системного промпта для адаптации ответов под пользователя.
- **Память диалога** — учёт ограниченного контекста предыдущих реплик для поддержания связного разговора.
- **Просмотр истории** — вывод на экран полной истории текущей сессии по команде.
- **Интерактивный CLI** — удобное взаимодействие с ботом через интерфейс командной строки с цветным выводом.
- **Локальная LLM** — работа с моделью `llama3:8b` через локальный сервер Ollama, что не требует внешних API-ключей.

---

## 🚀 Установка

### Требования

- Python 3.10+
- pip
- Docker

### Инструкция

1.  **Установите и запустите Ollama:**

    Для работы приложения требуется запущенный контейнер Docker с Ollama.

    ```bash
    # Первый запуск (создает контейнер)
    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

    # Последующие запуски (стартует существующий контейнер)
    docker start ollama
    ```

2.  **Загрузите модель Llama 3:**

    После запуска контейнера выполните команду для загрузки модели.

    ```bash
    docker exec -it ollama ollama run llama3:8b
    ```
    Дождитесь полного скачивания модели и закройте интерактивную сессию (Ctrl+D).

3.  **Клонируйте репозиторий:**

    ```bash
    git clone https://github.com/TihonSotnikov/GENAI-3-14.git
    cd GENAI-3-14
    ```

4.  **Создайте и активируйте виртуальное окружение:**

    ```bash
    # Создание
    python -m venv .venv

    # Активация (Windows PowerShell)
    .venv\Scripts\Activate.ps1

    # Активация (macOS/Linux)
    source .venv/bin/activate
    ```

5.  **Установите зависимости:**

    ```bash
    pip install -r requirements.txt
    ```

---

## 💻 Использование

Все взаимодействие с чат-ботом происходит через скрипт `cli.py`.

### Запуск сессии

Для начала диалога запустите скрипт:

```bash
python cli.py
```

При запуске скрипт предложит создать профиль: ввести имя и интересы. Эта информация будет использоваться для персонализации ответов в течение всей сессии.

### Команды в чате

-   `/history` — показать историю текущего диалога.
-   `q` или `quit` — завершить сессию.

---

## ⚙️ Настройка

Поведение бота можно гибко настраивать, изменяя параметры в файле `chatbot.py`.

-   **Глубина памяти:** Константа `DEFAULT_MAX_EXCHANGES` (по умолчанию `7`) определяет, сколько последних пар "вопрос-ответ" бот будет хранить в контексте диалога. Уменьшение этого значения сэкономит токены, а увеличение — позволит поддерживать более долгие и сложные беседы.
-   **Выбор модели:** В конструкторе класса `ChatBot` можно изменить используемую модель (`model="llama3:8b"`). Вы можете указать любую другую модель, предварительно загруженную в Ollama (например, `mistral`, `gemma:2b`).
-   **Адрес сервера:** Если ваш сервер Ollama запущен на другом адресе или порту, измените параметр `base_url="http://localhost:11434/v1"` в конструкторе.

> **⚠️ Важно:** Убедитесь, что Docker и контейнер `ollama` запущены перед запуском скрипта. Приложение обращается к `localhost:11434` и может работать некорректно при использовании **VPN** или **прокси-серверов**, блокирующих локальные соединения.

---

## 📁 Структура проекта

```
GENAI-3-14/
├── examples/               # Примеры работы утилиты
│   ├── example_dialog.txt
│   ├── example1.png
│   └── example2.png
├── .gitignore              # Файл для исключения версионирования
├── chatbot.py              # Основная логика: управление состоянием, промптами и API
├── cli.py                  # CLI-интерфейс для взаимодействия с пользователем
├── LICENSE                 # Файл лицензии (MIT)
├── README.md               # Документация проекта
└── requirements.txt        # Зависимости Python
```
---

## 📊 Пример сессии

Ниже приведен сокращенный пример диалога, демонстрирующий персонализацию и использование памяти.

```
──────────────────────────────────────────────────
Настройка профиля пользователя
    Введите ваше имя: Тихон
    Интересы и другая информация, которую бот должен знать: программист, живу в России, люблю арбузы
──────────────────────────────────────────────────

Бот готов. Для выхода введите 'q' или 'quit'. Для просмотра истории введите '/history'.

Вы: Привет!

Бот:
Привет, Тихон! Рад был тебя видеть. Как дела в мире программирования? У тебя есть интересные проекты, над которыми ты сейчас работает? И не забыл ли ты про свои любимые арбузы?

Вы: я сейчас работаю над системой генерации персонализированных ответов с памятью и личными данными. Про арбузы не забыл.

...

Вы: что ты обо мне знаешь?

Бот:
Тихон! Я знаю, что ты программист, живешь в России и любишь арбузы. Это основных фактов о тебе, которые я получил до начала нашего диалога. 
Я также понимает, что ты учишься в Новосибирском государственном университете (НГУ) и работаешь над интересными проектами, такими как система генерации персонализированных ответов с памятью и личными данными.
```
Полный лог этого диалога находится в файле `examples/example_dialog.txt`.

---

## 🎓 Образовательные цели

Проект направлен на освоение следующих концепций генеративного ИИ и разработки ПО:

-   **Управление состоянием (State Management)** — сохранение и использование данных пользователя и истории диалога в рамках одной сессии.
-   **Инженерия промптов (Prompt Engineering)** — динамическое конструирование системных сообщений для управления поведением модели.
-   **Контекстно-зависимая генерация** — учёт предыдущих реплик для поддержания связного разговора.
-   **Работа с локальными LLM** — интеграция и использование языковых моделей через Ollama без зависимости от облачных провайдеров.
-   **Проектирование CLI-приложений** — создание интерактивного и удобного интерфейса командной строки.

---

## 📄 Лицензия

Проект распространяется по лицензии [MIT](LICENSE).